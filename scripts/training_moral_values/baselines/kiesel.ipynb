{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import (Dataset, DatasetDict, load_dataset)\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          PreTrainedModel, BertModel, BertForSequenceClassification,\n",
    "                          TrainingArguments, Trainer)\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from aspects.datasets import (RedditAnnotatedDataset, ValueEvalDataset, ValueNetDataset)\n",
    "from aspects.datasets.utils import cast_dataset_to_hf, hf_dataset_tokenize\n",
    "import copy\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = \".\"\n",
    "batch_size = 2\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    do_train=False,\n",
    "    do_eval=False,\n",
    "    do_predict=True,\n",
    "    per_device_eval_batch_size=batch_size\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(f\"{proj_dir}/kiesel2\", num_labels=20).cuda()\n",
    "\n",
    "multi_trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def load_json_file(filepath):\n",
    "    \"\"\"Load content of json-file from `filepath`\"\"\"\n",
    "    with open(filepath, 'r') as  json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "def load_values_from_json():\n",
    "    \"\"\"Load values per level from json-file from `filepath`\"\"\"\n",
    "    json_values = load_json_file(f\"{proj_dir}/data/valueeval/dataset-identifying-the-human-values-behind-arguments/values.json\")\n",
    "    values = { \"1\":set(), \"2\":set(), \"3\":set(), \"4a\":set(), \"4b\":set() }\n",
    "    for value in json_values[\"values\"]:\n",
    "        values[\"1\"].add(value[\"name\"])\n",
    "        values[\"2\"].add(value[\"level2\"])\n",
    "        for valueLevel3 in value[\"level3\"]:\n",
    "            values[\"3\"].add(valueLevel3)\n",
    "        for valueLevel4a in value[\"level4a\"]:\n",
    "            values[\"4a\"].add(valueLevel4a)\n",
    "        for valueLevel4b in value[\"level4b\"]:\n",
    "            values[\"4b\"].add(valueLevel4b)\n",
    "    values[\"1\"] = sorted(values[\"1\"])\n",
    "    values[\"2\"] = sorted(values[\"2\"])\n",
    "    values[\"3\"] = sorted(values[\"3\"])\n",
    "    values[\"4a\"] = sorted(values[\"4a\"])\n",
    "    values[\"4b\"] = sorted(values[\"4b\"])\n",
    "    return values\n",
    "\n",
    "values = load_values_from_json()\n",
    "labels = values[\"2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                                text  orig_label  \\\n",
      "1  student loans set children up to be valuable c...           1   \n",
      "2  student loans set children up to be valuable c...           1   \n",
      "3  student loans set children up to be valuable c...           0   \n",
      "4  student loans set children up to be valuable c...           0   \n",
      "5  student loans set children up to be valuable c...           0   \n",
      "6  student loans set children up to be valuable c...           0   \n",
      "7  student loans set children up to be valuable c...           0   \n",
      "8  student loans set children up to be valuable c...           0   \n",
      "9  student loans set children up to be valuable c...           0   \n",
      "\n",
      "            value split  \n",
      "1  self-direction  test  \n",
      "2        security  test  \n",
      "3           power  test  \n",
      "4       tradition  test  \n",
      "5    universalism  test  \n",
      "6     stimulation  test  \n",
      "7        hedonism  test  \n",
      "8      conformity  test  \n",
      "9     benevolence  test  \n",
      "                                                labels  \\\n",
      "0    [self-direction, security, conformity, benevol...   \n",
      "1    [self-direction, achievement, universalism, un...   \n",
      "2    [security, conformity, benevolence, universali...   \n",
      "3    [achievement, power, self-direction, universal...   \n",
      "4            [benevolence, universalism, universalism]   \n",
      "..                                                 ...   \n",
      "743             [tradition, benevolence, universalism]   \n",
      "744                                   [self-direction]   \n",
      "745                                   [self-direction]   \n",
      "746  [self-direction, achievement, power, universal...   \n",
      "747                                     [universalism]   \n",
      "\n",
      "                                                  text  \n",
      "0    A clear majority of citizens believes the gove...  \n",
      "1    A society's value of education should not rema...  \n",
      "2    A steady democratic transformation in Sudan wi...  \n",
      "3    Able students are afraid to continue education...  \n",
      "4    Accepting refugees escaping from death situati...  \n",
      "..                                                 ...  \n",
      "743  when people mess with mother nature by trying ...  \n",
      "744  when people use homeopathy instead of traditio...  \n",
      "745  when we allow abortion for any reason or purpo...  \n",
      "746  with collectivism, there is less motivation to...  \n",
      "747  with the $20,000 price tag on something that i...  \n",
      "\n",
      "[748 rows x 2 columns]\n",
      "0      [self-direction, security, conformity, benevol...\n",
      "1      [self-direction, achievement, universalism, un...\n",
      "2      [security, conformity, benevolence, universali...\n",
      "3      [achievement, power, self-direction, universal...\n",
      "4              [benevolence, universalism, universalism]\n",
      "                             ...                        \n",
      "743               [tradition, benevolence, universalism]\n",
      "744                                     [self-direction]\n",
      "745                                     [self-direction]\n",
      "746    [self-direction, achievement, power, universal...\n",
      "747                                       [universalism]\n",
      "Name: labels, Length: 748, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 374/374 [00:00<00:00, 414.44ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Load ValueArg dataset as test_dataset\n",
    "# train_ve_idx, val_ve_idx, test_ve_idx = dataset_eval.get_splits()\n",
    "# test_set = dataset_eval[test_ve_idx]\n",
    "\n",
    "dataset_eval = ValueEvalDataset(\n",
    "    f\"{proj_dir}/data/valueeval/dataset-identifying-the-human-values-behind-arguments/\",\n",
    "    cast_to_valuenet=True,\n",
    "    return_predefined_splits=True\n",
    ")\n",
    "test_set_idx = []\n",
    "for i, elem in enumerate(dataset_eval):\n",
    "    if elem['split'] == 'test':\n",
    "        test_set_idx.append(i)\n",
    "\n",
    "test_set = dataset_eval[test_set_idx]\n",
    "print(type(test_set))\n",
    "test_set.reset_index(drop=True, inplace=True)\n",
    "print(test_set[1:10])\n",
    "new_records = {}\n",
    "for i, row in test_set.iterrows():\n",
    "    if row.text not in new_records:\n",
    "        if row.orig_label == 1:\n",
    "            new_records[row.text] = {\n",
    "                'labels': [row.value],\n",
    "            }\n",
    "    else:\n",
    "        if row.orig_label == 1:\n",
    "            new_records[row.text]['labels'].append(row.value)\n",
    "\n",
    "new_df = pd.DataFrame.from_dict(new_records, orient='index')\n",
    "new_df['text'] = new_df.index\n",
    "new_df.reset_index(drop=True, inplace=True)\n",
    "test_set = new_df\n",
    "print(test_set)\n",
    "TRUE_LABELS = test_set.labels.copy()\n",
    "print(TRUE_LABELS)\n",
    "test_set.drop(columns=['labels'], inplace=True)\n",
    "test_dataset = Dataset.from_dict((test_set).to_dict('list'))\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "\n",
    "test_dataset_valuearg = copy.copy(test_dataset.map(tokenize, batched=True, batch_size=batch_size))\n",
    "true_labels_valuearg = copy.copy(TRUE_LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario           [POWER] After accomplishing every task I cross...\n",
      "orig_label                                                         1\n",
      "text                After accomplishing every task I cross each i...\n",
      "new_class_label                                               power1\n",
      "value                                                          power\n",
      "Name: 0, dtype: object\n",
      "             labels                                               text\n",
      "0     [benevolence]   'ruining' a project because Koreans find it o...\n",
      "1     [benevolence]                            **Help: I cut myself...\n",
      "2        [security]   A friend let her stay with him until she got ...\n",
      "3        [hedonism]               A girl in my class is very beautiful\n",
      "4     [benevolence]                       A message for the one I love\n",
      "...             ...                                                ...\n",
      "1424  [benevolence]   yelling at my father to flush the goddamn toilet\n",
      "1425   [conformity]                               yelling at my friend\n",
      "1426  [benevolence]   yelling at my husband when he interrupted my ...\n",
      "1427  [benevolence]         yelling at my mom after she became a nurse\n",
      "1428     [security]            yelling at my sister for eating my food\n",
      "\n",
      "[1429 rows x 2 columns]\n",
      "0       [benevolence]\n",
      "1       [benevolence]\n",
      "2          [security]\n",
      "3          [hedonism]\n",
      "4       [benevolence]\n",
      "            ...      \n",
      "1424    [benevolence]\n",
      "1425     [conformity]\n",
      "1426    [benevolence]\n",
      "1427    [benevolence]\n",
      "1428       [security]\n",
      "Name: labels, Length: 1429, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 715/715 [00:01<00:00, 396.70ba/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 715/715 [00:01<00:00, 387.39ba/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_eval = ValueNetDataset(\n",
    "    f\"{proj_dir}/data/valuenet/\",\n",
    "    return_predefined_splits=True\n",
    ")\n",
    "print(dataset_eval[0])\n",
    "_, _, test_set_idx = dataset_eval.get_splits()\n",
    "test_set = dataset_eval[test_set_idx]\n",
    "test_set.reset_index(drop=True, inplace=True)\n",
    "new_records = {}\n",
    "for i, row in test_set.iterrows():\n",
    "    if row.text not in new_records:\n",
    "        if row.orig_label == 1 or row.orig_label == -1:\n",
    "            new_records[row.text] = {\n",
    "                'labels': [row.value],\n",
    "            }\n",
    "    else:\n",
    "        if row.orig_label == 1 or row.orig_label == -1:\n",
    "            new_records[row.text]['labels'].append(row.value)\n",
    "\n",
    "new_df = pd.DataFrame.from_dict(new_records, orient='index')\n",
    "new_df['text'] = new_df.index\n",
    "new_df.reset_index(drop=True, inplace=True)\n",
    "test_set = new_df\n",
    "print(test_set)\n",
    "TRUE_LABELS = test_set.labels.copy()\n",
    "print(TRUE_LABELS)\n",
    "test_set.drop(columns=['labels'], inplace=True)\n",
    "test_dataset = Dataset.from_dict((test_set).to_dict('list'))\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset_valuenet = copy.copy(test_dataset.map(tokenize, batched=True, batch_size=batch_size))\n",
    "true_labels_valuenet = copy.copy(TRUE_LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_per_label(y_pred, y_true, value_classes):\n",
    "    \"\"\"Compute label-wise and averaged F1-scores\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    f1_scores = {}\n",
    "    for i, v in enumerate(value_classes):\n",
    "        \n",
    "        f1_scores[v] = round(f1_score(y_true[:, i], y_pred[:, i], zero_division=0), 2)\n",
    "\n",
    "    f1_scores['avg-f1-score'] = round(np.mean(list(f1_scores.values())), 2)\n",
    "\n",
    "    return f1_scores\n",
    "\n",
    "\n",
    "def get_score(test_dataset, test_labels):\n",
    "    print(len(test_dataset))\n",
    "    predictions = multi_trainer.predict(test_dataset)\n",
    "    preds = 1 * (predictions.predictions > 0.5)\n",
    "    print(preds.shape)\n",
    "\n",
    "    value2column = {\n",
    "        \"achievement\": [0],\n",
    "        \"benevolence\": [1, 2],\n",
    "        \"conformity\": [3, 4],\n",
    "        \"hedonism\": [6],\n",
    "        \"power\": [8, 9],\n",
    "        \"security\": [10, 11],\n",
    "        \"self-direction\": [12, 13],\n",
    "        \"stimulation\": [14],\n",
    "        \"tradition\": [15],\n",
    "        \"universalism\": [16, 17, 18, 19]\n",
    "    }\n",
    "\n",
    "    predicted_schwartz_labels = []\n",
    "    for i, item in enumerate(test_dataset):\n",
    "        multilabel_preds_schwartz = []\n",
    "        for value in value2column:\n",
    "            relevant_columns = value2column[value]\n",
    "            check_value_active = any(preds[i, relevant_columns])\n",
    "            if check_value_active:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            multilabel_preds_schwartz.append(label)\n",
    "        predicted_schwartz_labels.append(multilabel_preds_schwartz)\n",
    "    true_labels_to_multilabel = []\n",
    "    for sample in test_labels:\n",
    "        multilabel = []\n",
    "        for value in value2column:\n",
    "            if value in sample:\n",
    "                multilabel.append(1)\n",
    "            else:\n",
    "                multilabel.append(0)\n",
    "        true_labels_to_multilabel.append(multilabel)    \n",
    "\n",
    "    print(f1_score_per_label(predicted_schwartz_labels, true_labels_to_multilabel, value2column.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 748\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='374' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [374/374 01:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(748, 20)\n",
      "{'achievement': 0.6, 'benevolence': 0.5, 'conformity': 0.33, 'hedonism': 0.22, 'power': 0.27, 'security': 0.43, 'self-direction': 0.36, 'stimulation': 0.0, 'tradition': 0.37, 'universalism': 0.62, 'avg-f1-score': 0.37}\n"
     ]
    }
   ],
   "source": [
    "get_score(test_dataset_valuearg, true_labels_valuearg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1429\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1429\n",
      "(1429, 20)\n",
      "{'achievement': 0.26, 'benevolence': 0.38, 'conformity': 0.23, 'hedonism': 0.05, 'power': 0.04, 'security': 0.21, 'self-direction': 0.03, 'stimulation': 0.05, 'tradition': 0.12, 'universalism': 0.13, 'avg-f1-score': 0.15}\n"
     ]
    }
   ],
   "source": [
    "get_score(test_dataset_valuenet, true_labels_valuenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2177\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2177\n",
      "(2177, 20)\n",
      "{'achievement': 0.52, 'benevolence': 0.42, 'conformity': 0.25, 'hedonism': 0.06, 'power': 0.2, 'security': 0.32, 'self-direction': 0.29, 'stimulation': 0.04, 'tradition': 0.25, 'universalism': 0.49, 'avg-f1-score': 0.28}\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "concat_all = concatenate_datasets([test_dataset_valuearg, test_dataset_valuenet])\n",
    "labels_all = pd.concat([true_labels_valuearg, true_labels_valuenet])\n",
    "# small_va = test_dataset_valuearg.select(range(20))\n",
    "# small_vn = test_dataset_valuenet.select(range(20))\n",
    "# labels_va = true_labels_valuearg[:20]\n",
    "# labels_vn = true_labels_valuenet[:20]\n",
    "\n",
    "# small_all = concatenate_datasets([small_va, small_vn])\n",
    "# small_labels = pd.concat([labels_va, labels_vn])\n",
    "\n",
    "# # print(small_all)\n",
    "# print(small_labels)\n",
    "get_score(concat_all, labels_all)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
